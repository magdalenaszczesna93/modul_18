{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.2. Budowa sieci neuronowej"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje aktywacyjne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # deklararujemy model\n",
    "# model = Sequential([\n",
    "#     Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
    "#     MaxPooling2D(),\n",
    "#     Conv2D(32, 3, padding='same', activation='relu'),\n",
    "#     MaxPooling2D(),\n",
    "#     Conv2D(64, 3, padding='same', activation='relu'),\n",
    "#     MaxPooling2D(),\n",
    "#     Flatten(),\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "\n",
    "# # kompilujemy model\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # wypisujemy wartości warstw modelu\n",
    "# model.summary()\n",
    "\n",
    "# # trenujemy model\n",
    "# history = model.fit_generator(\n",
    "#     train_data_gen,\n",
    "#     steps_per_epoch=total_train // batch_size,\n",
    "#     epochs=epochs,\n",
    "#     validation_data=val_data_gen,\n",
    "#     validation_steps=total_val // batch_size\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "# from ranger import Ranger  # this is from ranger.py\n",
    "# from ranger import RangerVA  # this is from ranger913A.py\n",
    "# from ranger import RangerQH  # this is from rangerqh.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sposób 1. Każdy optimizer posiada swoją nazwę, która go w pełni identyfikuje. Aby go użyć z domyślnymi parametrami, wystarczy podczas kompilacji modelu jako optimizer wpisać tę nazwę.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #przykładowy optimizer z tensorflow\n",
    "# tf.keras.optimizers.Adam(\n",
    "#     learning_rate=0.001,\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.999,\n",
    "#     epsilon=1e-07,\n",
    "#     amsgrad=False,\n",
    "#     name='Adam',\n",
    "#     **kwargs\n",
    "# )\n",
    "\n",
    "# # przekazujemy nazwę, która identyfikuje nasz optimizer\n",
    "# model.compile(optimizer='adam', ...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sposób 2. Jeżeli chcemy zmienić jakąkolwiek wartość (np. learning_rate), powinniśmy przekazać nowy obiekt optimizera do metody compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tworzymy obiekt typu Adam z naszym learning rate\n",
    "# adam_optim = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# # tworzymy model\n",
    "# # ...\n",
    "\n",
    "# # przekazujemy obiekt zamiast nazwy\n",
    "# model.compile(optimizer=adam_optim, ...)\n",
    "\n",
    "# # szkolimy model\n",
    "# # ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(\n",
    "#     ...,\n",
    "#     callbacks=[\n",
    "#         tf.keras.callbacks.LearningRateScheduler(wlasna_funkcja)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Najprostszym sposobem, aby uzyskać zmianę learning_rate jest ustawienie parametru decay \n",
    "# # w optimizerach (jest to parametr klasy bazowej, więc trzeba go szukać tutaj).\n",
    "# adam_opt = keras.optimizers.Adam(lr=0.01, decay=1e-5)\n",
    "\n",
    "# # Exponential scheduling ze stałą wartością learning_rate – zmiana wartości learning_rate \n",
    "# # wykładniczo:\n",
    "# def exponential_decay_fn(epoch):\n",
    "# \treturn 0.05 * 0.2**(epoch / 10)\n",
    "\n",
    "# # Exponential scheduling z wartością zależną od aktualnego learning_rate:\n",
    "# def exponential_decay_fn(epoch, current_lr):\n",
    "#   return current_lr * 0.2**(epoch / 10)\n",
    "\n",
    "# # Scheduler zależny od numeru epoki:\n",
    "# # - dla pierwszych epok ustaw duża wartość\n",
    "# # - dla epok do 20 ustaw mniejszą wartość\n",
    "# # - dla pozostałych ustaw małą wartość\n",
    "# def const_scheduler(epoch):\n",
    "#     if epoch < 10:\n",
    "#           return 0.05\n",
    "#     elif epoch < 20:\n",
    "#           return 0.005\n",
    "#     else:\n",
    "#           return 0.0005"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions – funkcje błędu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy wzór jest pokazany tylko w celach informacyjnych (oraz dlatego, że na każdej rozmowie o pracę pada pytanie o to, jaki wzór ma ta funkcja :)).\n",
    "\n",
    "### L = -y*log(p) - (1-y) * log(1-p)\n",
    "\n",
    "\n",
    "L – Loss, wartość błędu\n",
    "\n",
    "y – wartość prawdziwa (numer klasy)\n",
    "\n",
    "p – prawdopodobieństwo wyznaczone przez model dla danej klasy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labels)\n",
    "\n",
    "# # [9 0 0 ... 3 0 5]\n",
    "\n",
    "# print(\"spase: \", labels[0])\n",
    "# one_hot_label = tf.keras.utils.to_categorical(labels[0])\n",
    "# print(\"one hot encoded \", one_hot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.losses.CategoricalCrossentropy(\n",
    "#     from_logits=False,\n",
    "#     label_smoothing=0,\n",
    "#     reduction=losses_utils.ReductionV2.AUTO,\n",
    "#     name='categorical_crossentropy'\n",
    "# )\n",
    "\n",
    "# #przekazujemy nazwę, która identyfikuje naszą funkcję\n",
    "# model.compile(..., loss='categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tworzymy obiekt CategoricalCrossEntropy\n",
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# # tworzymy model\n",
    "# # ...\n",
    "\n",
    "# # przekazujemy obiekt zamiast nazwy\n",
    "# model.compile(..., loss=loss_fn)\n",
    "\n",
    "# # szkolimy model\n",
    "# #..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie: model klasyfikacji\n",
    "Stwórz model klasyfikacji, w oparciu o architekturę sieci neuronowych. Naucz model klasyfikować gatunki kwiatów, wykorzystując zbiór Iris. Pamiętaj, że jest to klasyfikacja wieloklasowa, w związku z tym w ostatniej warstwie sieci powinna być funkcja aktywacji Softmax.\n",
    "\n",
    "Prześlij Mentorowi link do swojego Notatnika w Jupyterze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "# import seaborn as sns\n",
    "# iris = sns.load_dataset('iris')\n",
    "X, y, iris_classes = iris.data, iris.target, iris.target_names\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n",
    "                                    random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_97 (Dense)            (None, 4, 16)             32        \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 4, 32)             544       \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 4, 64)             2112      \n",
      "                                                                 \n",
      " flatten_44 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,785\n",
      "Trainable params: 134,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "4/4 [==============================] - 3s 106ms/step - loss: 0.0000e+00 - accuracy: 0.3333 - val_loss: 0.0000e+00 - val_accuracy: 0.3333 - lr: 0.0500\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.3333 - val_loss: 0.0000e+00 - val_accuracy: 0.3333 - lr: 0.0500\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.3583 - val_loss: 0.0000e+00 - val_accuracy: 0.6000 - lr: 0.0500\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.6500 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0500\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0500\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0500\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0500\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0500\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0500\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0500\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.6667 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "def const_scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "          return 0.05\n",
    "    elif epoch < 20:\n",
    "          return 0.005\n",
    "    else:\n",
    "          return 0.0005\n",
    "    \n",
    "# deklararujemy model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=[4,1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(64, activation='relu')),\n",
    "model.add(Flatten()),\n",
    "model.add(Dense(512, activation='softmax')),\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.build((None, 4))\n",
    "\n",
    "# kompilujemy model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# wypisujemy wartości warstw modelu\n",
    "model.summary()\n",
    "\n",
    "callback=[tf.keras.callbacks.LearningRateScheduler(const_scheduler)]\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=200, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.3. Podstawowe warstwy sieci neuronowych"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dense_layer = tf.keras.layers.Dense(32)\n",
    "\n",
    "# albo bardziej dosłownie\n",
    "\n",
    "dense_layer = tf.keras.layers.Dense(units=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = tf.keras.layers.Flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie sieci neuronowej w TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = nasz_model(dane_wejsciowe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]\n",
      " [ 0.7388872  0.8521431 -1.3374448  1.0928134]], shape=(16, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]\n",
      " [-0.16735505 -0.05366759 -0.8764189   0.738747  ]], shape=(16, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.layers as layers\n",
    "\n",
    "input_data = tf.ones((16, 3, 3))\n",
    "\n",
    "#stworzenie modelu\n",
    "seq_model = tf.keras.Sequential()\n",
    "\n",
    "# sposób pierwszy\n",
    "\n",
    "# input_shape jest niewymagane, lecz pozwala powiedzieć,\n",
    "# \"chcę mieć taki rozmiar danych wejściowych\", co pozwala\n",
    "# uniknąć głupich pomyłek w stylu: przekazujemy inny rozmiar,\n",
    "# bo zapomnieliśmy np. zmniejszyć obrazów wejściowych\n",
    "seq_model.add(tf.keras.layers.Flatten(input_shape=[3, 3]))\n",
    "seq_model.add(tf.keras.layers.Dense(units=16, name='input_layer'))\n",
    "seq_model.add(tf.keras.layers.Dense(units=32, name='hidden_layer'))\n",
    "seq_model.add(tf.keras.layers.Dense(units=4, name='output_layer'))\n",
    "\n",
    "# sposób drugi\n",
    "seq_model_2 = tf.keras.Sequential([\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(16, name=\"input_layer\"),\n",
    "    layers.Dense(32, name=\"hidden_layer\"),\n",
    "    layers.Dense(4, name=\"output_layer\")\n",
    "])\n",
    "\n",
    "# uruchomienie naszego modelu z wygenerowanymi danymi\n",
    "output = seq_model(input_data)\n",
    "print(output)\n",
    "output2 = seq_model_2(input_data)\n",
    "print(output2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 23s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 4s 1us/step\n"
     ]
    }
   ],
   "source": [
    "tf.keras.datasets.fashion_mnist.load_data()\n",
    "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# wydobycie obrazów oraz labelek\n",
    "images, labels = train\n",
    "\n",
    "# normalizacja wartości pikseli (maks. wartość wynosi 255.0, \n",
    "# czyli aby znormalizować nasze dane, musimy podzielić każdy piksel przez maks. wartość)\n",
    "images = images/255.0\n",
    "\n",
    "# zapisujemy dane jako int\n",
    "labels = labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.1, random_state=10, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stworzenie zbioru typu Dataset z naszej listy\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "\n",
    "# ustawienie batch_size na 32 oraz przetasowanie na bazie 1000 próbek\n",
    "train_ds = train_ds.shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_50 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 200)               60200     \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 150)               30150     \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 341,960\n",
      "Trainable params: 341,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#stworzenie modelu\n",
    "f_mnist_model = tf.keras.Sequential([\n",
    "    # spłaszczanie obrazka do wektora jednowymiarowego\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dense(200, activation='relu'),\n",
    "    layers.Dense(150, activation='relu'),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    \n",
    "    # ostatnia warstwa posiada tyle neuronów ile mamy klas\n",
    "    layers.Dense(10, activation='softmax')])\n",
    "\n",
    "f_mnist_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kompilacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mnist_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Szkolenie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 9s 4ms/step - loss: 0.5005 - accuracy: 0.8175\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3757 - accuracy: 0.8613\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3398 - accuracy: 0.8758\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3134 - accuracy: 0.8821\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2947 - accuracy: 0.8904\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2813 - accuracy: 0.8946\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2670 - accuracy: 0.8994\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2563 - accuracy: 0.9042\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2486 - accuracy: 0.9062\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2368 - accuracy: 0.9096\n"
     ]
    }
   ],
   "source": [
    "train_stats = f_mnist_model.fit(train_ds, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(train_stats.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 1s 2ms/step\n",
      "probs :  [5.1412330e-26 0.0000000e+00 9.2706207e-34 1.9538803e-33 1.2226658e-32\n",
      " 1.0000000e+00 7.8216251e-30 2.2317687e-17 1.1589095e-31 9.2007881e-20]\n",
      "klasa : 5\n",
      "rzeczywista klasa:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprobs :  [4.0548810e-15 1.0000000e+00 9.3477974e-17 5.3090128e-13 7.5702587e-15\\n 5.9295928e-25 2.1536054e-11 3.4459677e-24 2.8725664e-16 2.4974258e-22]\\nklasa : 1\\nrzeczywista klasa:  1\\n'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = f_mnist_model.predict(X_test)\n",
    "print(\"probs : \", y_pred[2])\n",
    "print(\"klasa :\", np.argmax(y_pred[2]))\n",
    "print(\"rzeczywista klasa: \", y_test[2])\n",
    "\n",
    "\"\"\"\n",
    "probs :  [4.0548810e-15 1.0000000e+00 9.3477974e-17 5.3090128e-13 7.5702587e-15\n",
    " 5.9295928e-25 2.1536054e-11 3.4459677e-24 2.8725664e-16 2.4974258e-22]\n",
    "klasa : 1\n",
    "rzeczywista klasa:  1\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API do stworzenia modelu w TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model implementujący Residual Layer za pomocą Functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 28, 28)]     0           []                               \n",
      "                                                                                                  \n",
      " flatten_51 (Flatten)           (None, 784)          0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " hidden_1 (Dense)               (None, 320)          251200      ['flatten_51[0][0]']             \n",
      "                                                                                                  \n",
      " hidden_2 (Dense)               (None, 150)          48150       ['hidden_1[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 934)          0           ['flatten_51[0][0]',             \n",
      "                                                                  'hidden_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_119 (Dense)              (None, 10)           9350        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 308,700\n",
      "Trainable params: 308,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# stworzenie wejścia\n",
    "\n",
    "input = tf.keras.Input(shape=X_train.shape[1:])\n",
    "\n",
    "# możemy wypisać, co ta warstwa przyjmuje - jest to rozmiar\n",
    "# naszego obrazka bez batch_size (który ma wartość None)\n",
    "print(input)\n",
    "\n",
    "# spłaszczenie wejścia\n",
    "input_flat = layers.Flatten(input_shape=[28,28])(input)\n",
    "\n",
    "# nasza kolejna warstwa jest typu Dense, jak poprzednio, ale od razu\n",
    "# i bezpośrednio przekazujemy jej wejście, tak jak funkcji w Pythonie:\n",
    "hidden_1 =layers.Dense(320, activation='relu', name=\"hidden_1\")(input_flat)\n",
    "hidden_2 =layers.Dense(150, activation='relu', name=\"hidden_2\")(hidden_1)\n",
    "\n",
    "# złączamy wyniki z obu warstw za pomocą warstwy typu Concatenate\n",
    "concat_layer = layers.Concatenate()([input_flat, hidden_2])\n",
    "output = layers.Dense(10, activation='softmax')(concat_layer)\n",
    "\n",
    "# tworzymy model, przekazując mu co ma być naszymi wyjściami, a co wejściami\n",
    "model_res = tf.keras.Model(inputs=[input], outputs=[output])\n",
    "\n",
    "# podsumowanie naszego modelu\n",
    "model_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 8s 4ms/step - loss: 0.4858 - accuracy: 0.8250\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3636 - accuracy: 0.8660\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3292 - accuracy: 0.8781\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3027 - accuracy: 0.8871\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2841 - accuracy: 0.8934\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.2721 - accuracy: 0.8984\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2550 - accuracy: 0.9026\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2460 - accuracy: 0.9066\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2346 - accuracy: 0.9105\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2240 - accuracy: 0.9139\n"
     ]
    }
   ],
   "source": [
    "# kompilacja\n",
    "model_res.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#szkolenie na takich samych danych jak poprzednio\n",
    "train_stats = model_res.fit(train_ds, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stwórzmy teraz drugi model, z dwoma wyjściami dla tekstu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pamiętajmy, aby nie dodawać pierwszego wymiaru (batch_size)\n",
    "text = np.array([[\"ala ma kota\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_120 (Dense)              (None, 320)          640         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_121 (Dense)              (None, 150)          48150       ['dense_120[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 151)          0           ['input_2[0][0]',                \n",
      "                                                                  'dense_121[0][0]']              \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 28, 28)]     0           []                               \n",
      "                                                                                                  \n",
      " dense_122 (Dense)              (None, 10)           1520        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 50,310\n",
      "Trainable params: 50,310\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text = np.array([[\"ala ma kota\"]])\n",
    "\n",
    "input_1 = tf.keras.Input(shape=text.shape[1:])\n",
    "input_2 = tf.keras.Input(shape=X_train.shape[1:])\n",
    "\n",
    "# nasza kolejna warstwa jest typu Dense, jak poprzednio,\n",
    "# ale od razu przekazujemy jej wejście, tak jak funkcji w Pythonie\n",
    "hidden_1 =layers.Dense(320, activation='relu')(input_1)\n",
    "hidden_2 =layers.Dense(150, activation='relu')(hidden_1)\n",
    "\n",
    "# złączamy wyniki naszych warstw za pomocą warstwy\n",
    "# typu Concatenate podając jako argumenty input_1 oraz hidden_2\n",
    "concat_layer = layers.Concatenate()([input_1, hidden_2])\n",
    "output = layers.Dense(10, activation='softmax')(concat_layer)\n",
    "\n",
    "# tworzymy model, przekazując mu co ma być naszymi wyjściami, a co wejściami\n",
    "model = tf.keras.Model(inputs=[input_1, input_2], outputs=[output])\n",
    "\n",
    "# podsumowanie naszego modelu\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a491fa038a24a3354ef15b8320e5eed1f98c46448a463343d2ea596d5b86218b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
